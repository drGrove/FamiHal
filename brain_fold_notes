## Initial Interface

digraph a {
Asterisk->GoogleSpeechRecog->TextInput
{RSS,IRC,hyve}->TextInput
TextInput->SentimentClassifier
TextInput->Markov
SentimentClassifier->MarkovChains
MarkovChains->GoogleTTS
}

The Asterisk sever serves as an IVR to chunk open-air conversations into processable audio chunks.
The audio chunks are processed by the google speech recognition webapi. Tonal inflections are ignored, and the speech is treated as plain text input.
The RSS feed text input acts as HAL's "reading habits". These should reflect news sources familab members follow, since HAL should seem approachable and relatable. This becomes text input. 
HAL will have a presence in IRC, at this stage he will only listen and learn IRC patterns of speech. THis becomes text input. 
HAL will follow social media over a selection of keywords using hyve. He will not follow anyone personally in this version. This becomes text input. 

All text input is lowercased, contractions are expanded, and punctuation is removed.

This processed text is given a sentiment value by being passed through two bayesian classifiers: one positive, one negative. 

This processed text is split into 3-gram chains and added to the Redis Markov Chain server. 

Any input is considered justification for response, so HAL will issue a response as soon as he's given an input. This response is generated by recursive query over the Redis Markov Chain server. A response is attempted 300 times or until a chain is created that matches the sentiment of the text input. 

This text is passed into the google text-to-speech webapi and spoken out of HAL. 

There is no IRC or social media output planned for this version. 




## Overall Plan

digraph a {
{GoogleSpeechRecog,IRC,RSS,hyve}->IdentitySystem
Asterisk->GoogleSpeechRecog->TextInput
{RSS,IRC,hyve}->TextInput
TextInput->SentimentClassifier
TextInput->MarkovChains
SentimentClassifier->MarkovChains
MarkovChains->GoogleTTS
TextInput->ConceptSpace->MarkovChains
SentimentClassifier->EmotionEngine
IdentitySystem->MarkovChains
{IdentitySystem,ConceptSpace}->StateTracker->MarkovChains
EmotionEngine->MarkovChains
}

This plan is the same as the above plan, but expanded in the following ways.

Social media is now gathered from member accounts. 

All forms of input that are associated with lab members have their stanzas passed into the IdentitySystem. The data that is passed into the IdentitySystem from IRC and social media are used to train the IdentitySystem's classifier. Stanzas from RSS, hyve, and GoogleSpeechRecog are tagged with 'related members'. Each piece of text information received will have a list of numbers that tell how relevant this text is to each member of familab. 

All text inputs are still treated as regular text inputs,passed through sentiment classification, and into markov chains.

Whenever the SentimentClassifier is used to learn new text, it modifies the EmotionEngine, positively or negative. The EmotionEngine only has 1 dimension of emotion in this version. The initial version simply tries to replicate the sentiment of the incoming text, this version has an inertia to emotion which is carried out by the EmotionEngine. 

All text input is passed into the ConceptSpace to assign it metadata about which topic it is in reference to. The ConceptSpace is generated by a Latent Dirichlet Analysis implementation, retrained every hour. 

The StateTracker is the part of the application that tracks conversations. The StateTracker uses the identity information of text pieces and their concepts to draw a network of concepts currently being talked about, and by who. It will analyze the network using the reduced PageRank algorithm (multiply the matrix by itself ~34 times, take convergent values as column vector). Send this state along to the Markov Chain generator. 

All text input is used to train the Markov Chain Generator. 

The Markov Chain Generator sldo decides what to speak. It does this by taking into account it's current emotional state (EmotionEngine), who it feels it is currently talking to in this response (IdentitySystem), what the current topics are for that person (StateTracker), and what words might be valuable to expressing those ideas. A sigmoid function is used to choose the top topics from StateEngine, then words from ConceptSpace. We generate 300 potential markov chains, put them through the ConceptSpace to give them concept values, and choose the most relevant statement that reflects the emotion and identity. 

This statement is passed back to the instigating message's form of communication: either IRC or text-to-speech. 





















